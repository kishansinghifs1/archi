# Basic configuration file for a Archi deployment
# with a chat app interface, a QA pipeline, and
# PostgreSQL with pgvector for document storage.
# The LLM is used through an existing Ollama server.
#
# run with:
# archi create --name my-archi-ollama --config examples/deployments/basic-ollama/config.yaml --services chatbot --hostmode

name: my_archi

services:
  chat_app:
    pipeline: CMSCompOpsAgent
    trained_on: "My data"
    port: 7866
    external_port: 7866
  vectorstore:
    backend: postgres  # PostgreSQL with pgvector (only supported backend)
  data_manager:
    port: 7889
    external_port: 7889

data_manager:
  sources:
    jira:
      max_tickets: 10
      url: https://its.cern.ch/jira/
      projects:
        - "CMSPROD"
    links:
      input_lists:
        - examples/deployments/basic-ollama/miscellanea.list
    redmine:
      url: https://cleo.mit.edu
      projects:
        - emails-to-ticket
  embedding_name: HuggingFaceEmbeddings

archi:
  pipelines:
    - CMSCompOpsAgent
  providers:
    local:
      enabled: true
      base_url: http://localhost:7870 # make sure this matches your ollama server URL!
      mode: ollama
      default_model: "qwen3:32b" # make sure this matches a model you have downloaded locally with ollama
      models:
        - "gpt-oss:120b"
        - "qwen3:32b"
  pipeline_map:
    CMSCompOpsAgent:
      prompts:
        required:
          agent_prompt: examples/deployments/basic-ollama/agent.prompt
      models:
        required:
          agent_model: local/qwen3:32b
